---
title: "Generalized Kumaraswamy Regression Models"
author: "Lopes, J. E."
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Generalized Kumaraswamy Regression Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  fig.align = "center",
  warning = FALSE,
  message = FALSE
)
library(gkwreg)
library(TMB)
library(patchwork)
```

# Introduction

The `gkwreg` package implements statistical models based on the Generalized Kumaraswamy (GKw) distribution for continuous data bounded in the open unit interval (0,1). This package provides two main components:

1. **Distribution fitting** (`gkwfit`): Maximum likelihood estimation for all seven models in the GKw family
2. **Regression modeling** (`gkwreg`): A framework that models all five distribution parameters as functions of covariates

The implementations use C++ with RcppArmadillo and Template Model Builder (TMB) for efficient parameter estimation via automatic differentiation.

## Motivation

Data bounded in the (0,1) interval are common in various fields:

- Rates and proportions (e.g., infection rates, voting proportions)
- Normalized indices (e.g., economic indices, environmental measures)
- Probabilities (e.g., chance of an event)
- Fractional measures (e.g., chemical concentrations)

While beta-regression models are popular for this type of data, the Generalized Kumaraswamy distribution offers greater flexibility to capture various shapes and asymmetries, with the ability to model unimodal, bimodal, J-shaped, inverted J-shaped, U-shaped distributions, with different levels of kurtosis and skewness.

# Mathematical Foundation

## Special Functions

Before introducing the distributions, we briefly review the special functions used:

### Gamma Function

The Gamma function is an extension of the factorial for real and complex numbers:

$$\Gamma(a) = \int_0^{\infty} t^{a-1}e^{-t}dt, \quad a > 0$$

Important properties:
- $\Gamma(a+1) = a\Gamma(a)$
- $\Gamma(n) = (n-1)!$ for positive integer $n$
- $\Gamma(1/2) = \sqrt{\pi}$

### Beta Function

The Beta function is defined in terms of the Gamma function:

$$B(a,b) = \frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)} = \int_0^1 t^{a-1}(1-t)^{b-1}dt, \quad a,b > 0$$

### Regularized Incomplete Beta Function

The regularized incomplete Beta function is defined as:

$$I_x(a,b) = \frac{1}{B(a,b)}\int_0^x t^{a-1}(1-t)^{b-1}dt, \quad 0 \leq x \leq 1$$

### Digamma Function

The Digamma function (ψ) is the logarithmic derivative of the Gamma function:

$$\psi(a) = \frac{d}{da}\ln\Gamma(a) = \frac{\Gamma'(a)}{\Gamma(a)}$$

### Trigamma Function

The Trigamma function (ψ') is the derivative of the Digamma function:

$$\psi'(a) = \frac{d^2}{da^2}\ln\Gamma(a) = \frac{d}{da}\psi(a)$$

## The Generalized Kumaraswamy Distribution (GKw)

The GKw distribution, introduced by Cordeiro and de Castro (2011), is a flexible five-parameter family for bounded continuous random variables. For a random variable $Y \in (0,1)$, the probability density function (PDF) of the GKw distribution is:

$$f(y; \alpha, \beta, \gamma, \delta, \lambda) = \frac{\lambda \alpha \beta y^{\alpha-1} (1-y^\alpha)^{\beta-1}}{B(\gamma, \delta+1)} \Bigl[1-(1-y^\alpha)^\beta\Bigr]^{\gamma\lambda-1} \Bigl\{1-\Bigl[1-(1-y^\alpha)^\beta\Bigr]^\lambda\Bigr\}^{\delta}$$

where $\alpha, \beta, \gamma, \delta, \lambda > 0$ and $B(\gamma, \delta+1)$ is the complete Beta function.

The cumulative distribution function (CDF) is:

$$F(y; \alpha, \beta, \gamma, \delta, \lambda) = I_{[1-(1-y^\alpha)^\beta]^\lambda}(\gamma, \delta+1)$$

where $I_z(a,b)$ is the regularized incomplete Beta function.

### Auxiliary Transformations

To simplify derivations, we define the following transformations:

- $A = y^{\alpha}$
- $v = 1 - A = 1 - y^{\alpha}$
- $w = 1 - v^{\beta} = 1 - (1 - y^{\alpha})^{\beta}$
- $z = 1 - w^{\lambda} = 1 - [1 - (1 - y^{\alpha})^{\beta}]^{\lambda}$

### GKw Log-Likelihood

The log-likelihood function for a sample of size $n$ is:

$$\ell(\theta) = n\ln(\lambda) + n\ln(\alpha) + n\ln(\beta) - n\ln B(\gamma, \delta+1) + (\alpha-1) \sum_{i=1}^n \ln(y_i) + (\beta-1) \sum_{i=1}^n \ln(1 - y_i^\alpha) + (\gamma\lambda - 1) \sum_{i=1}^n \ln\{1 - (1 - y_i^\alpha)^\beta\} + \delta \sum_{i=1}^n \ln\{1 - \{1 - (1 - y_i^\alpha)^\beta\}^\lambda\}$$

where $\theta = (\alpha, \beta, \gamma, \delta, \lambda)$ is the parameter vector.

## Distribution Hierarchy

The GKw distribution encompasses a hierarchy of nested distributions through parameter constraints:

### 1. Generalized Kumaraswamy Distribution (GKw)

**Parameters**: $\alpha, \beta, \gamma, \delta, \lambda > 0$

**Density**:
$$f(y;\theta) = \frac{\lambda\alpha\beta}{B(\gamma,\delta+1)}y^{\alpha-1}(1 - y^\alpha)^{\beta-1}[1 - (1 - y^\alpha)^\beta]^{\gamma\lambda-1}\{1 - [1 - (1 - y^\alpha)^\beta]^\lambda\}^\delta$$

### 2. Beta-Kumaraswamy Distribution (BKw)

**Parameters**: $\alpha, \beta, \gamma, \delta > 0$, with $\lambda = 1$ fixed

**Density**:
$$f(y) = \frac{\alpha\beta}{B(\gamma,\delta+1)}y^{\alpha-1}(1 - y^\alpha)^{\beta(\delta+1)-1}[1 - (1 - y^\alpha)^\beta]^{\gamma-1}$$

### 3. Kumaraswamy-Kumaraswamy Distribution (KKw)

**Parameters**: $\alpha, \beta, \delta, \lambda > 0$, with $\gamma = 1$ fixed

**Density**:
$$f(y) = \lambda\alpha\beta(\delta+1)y^{\alpha-1}(1 - y^\alpha)^{\beta-1}[1 - (1 - y^\alpha)^\beta]^{\lambda-1}\{1 - [1 - (1 - y^\alpha)^\beta]^\lambda\}^\delta$$

### 4. Exponentiated Kumaraswamy Distribution (EKw)

**Parameters**: $\alpha, \beta, \lambda > 0$, with $\gamma = 1, \delta = 0$ fixed

**Density**:
$$f(y) = \lambda\alpha\beta y^{\alpha-1}(1 - y^\alpha)^{\beta-1}[1 - (1 - y^\alpha)^\beta]^{\lambda-1}$$

### 5. McDonald Distribution (Mc) or Beta Power

**Parameters**: $\gamma, \delta, \lambda > 0$, with $\alpha = 1, \beta = 1$ fixed

**Density**:
$$f(y) = \frac{\lambda\gamma}{B(\gamma,\delta+1)}y^{\lambda\gamma-1}(1 - y^\lambda)^\delta$$

### 6. Kumaraswamy Distribution (Kw)

**Parameters**: $\alpha, \beta > 0$, with $\gamma = 1, \delta = 0, \lambda = 1$ fixed

**Density**:
$$f(y) = \alpha\beta y^{\alpha-1}(1 - y^\alpha)^{\beta-1}$$

### 7. Beta Distribution

**Parameters**: $\gamma, \delta > 0$, with $\alpha = 1, \beta = 1, \lambda = 1$ fixed

**Density**:
$$f(y) = \frac{1}{B(\gamma,\delta+1)}y^{\gamma-1}(1 - y)^\delta$$

This nested structure enables model selection through likelihood ratio tests, starting with the most complex model (GKw) and potentially simplifying to more parsimonious forms when appropriate.

# Regression Framework

## GKw Regression Model

The GKw regression model extends the GKw distribution by modeling each parameter as a function of covariates:

$$\theta_i = g_\theta^{-1}(\mathbf{x}_{\theta i}^T \boldsymbol{\beta}_\theta), \quad \theta \in \{\alpha, \beta, \gamma, \delta, \lambda\}$$

where:
- $\mathbf{x}_{\theta i}$ is the covariate vector for observation $i$ and parameter $\theta$
- $\boldsymbol{\beta}_\theta$ is the corresponding coefficient vector
- $g_\theta^{-1}$ is the inverse link function for parameter $\theta$

## Link Functions

The package supports nine inverse link functions:

1. **Log**: $g^{-1}(\eta) = \exp(\eta)$
2. **Logit**: $g^{-1}(\eta) = \text{scale} \cdot \frac{\exp(\eta)}{1+\exp(\eta)}$
3. **Probit**: $g^{-1}(\eta) = \text{scale} \cdot \Phi(\eta)$
4. **Cauchy**: $g^{-1}(\eta) = \text{scale} \cdot \Bigl(\frac{1}{\pi}\arctan(\eta) + \frac{1}{2}\Bigr)$
5. **Cloglog**: $g^{-1}(\eta) = \text{scale} \cdot \bigl(1 - \exp(-\exp(\eta))\bigr)$
6. **Identity**: $g^{-1}(\eta) = \eta$
7. **Square Root**: $g^{-1}(\eta) = \eta^2$
8. **Inverse**: $g^{-1}(\eta) = \frac{1}{\eta}$
9. **Inverse-Square**: $g^{-1}(\eta) = \frac{1}{\sqrt{\eta}}$

The `scale` parameter allows rescaling bounded link functions (logit, probit, cauchy, cloglog) to appropriate parameter ranges.

## Model Specification via Formula

The model is specified through a multi-part formula:

```r
y ~ alpha_terms | beta_terms | gamma_terms | delta_terms | lambda_terms
```

Each right-hand side part corresponds to the linear predictor for the respective parameter. If fewer than five parts are provided, the missing parts are treated as intercept-only models.

# Distribution Fitting with gkwfit()

The `gkwfit()` function implements maximum likelihood estimation for the GKw distribution family. It supports two estimation methods: Template Model Builder (TMB) and Newton-Raphson (NR).

## Basic Usage

```{r, eval=FALSE}
# Generate sample data (from a Beta distribution for simplicity)
set.seed(123)
x <- rbeta(200, shape1 = 2, shape2 = 5)

# Fit full GKw model
fit_gkw <- gkwfit(x, family = "gkw", profile = TRUE, plot = TRUE)

# Fit a simpler Kumaraswamy model
fit_kw <- gkwfit(x, family = "kw", , profile = TRUE, plot = TRUE)

# Examine results
summary(fit_gkw)
summary(fit_kw)

# Compare models with AIC/BIC
AIC(fit_gkw, fit_kw)
BIC(fit_gkw, fit_kw)

# Visualize fit
fit_kw$plots
```

## Key Parameters

```r
gkwfit(data, family = c("gkw", "bkw", "kkw", "ekw", "mc", "kw", "beta"),
       start = NULL, fixed = NULL, fit = c("nr", "tmb"),
       method = c("nlminb", "optim"), use_moments = FALSE,
       hessian = TRUE, profile = FALSE, npoints = 20,
       plot = TRUE, conf.level = 0.95, optimizer.control = list(),
       submodels = FALSE, silent = FALSE, ...)
```

- `data`: Numeric vector with values in the (0, 1) interval
- `family`: Distribution family (one of "gkw", "bkw", "kkw", "ekw", "mc", "kw", "beta")
- `start`: Optional list with initial parameter values
- `fixed`: Optional list of parameters to be held fixed (not estimated)
- `fit`: Estimation method ("tmb" or "nr")
- `method`: Optimization method when using TMB ("nlminb" or "optim")
- `use_moments`: Logical; if TRUE, uses method of moments for initial values
- `hessian`: Logical; if TRUE, computes standard errors and covariance matrix
- `profile`: Logical; if TRUE, computes likelihood profiles for parameters
- `submodels`: Logical; if TRUE, fits nested submodels for comparison
- `plot`: Logical; if TRUE, generates diagnostic plots

## Example: Comparing Distribution Families

```{r, eval=FALSE}
# Generate data from a mixture of two beta distributions
set.seed(4567)
x <- c(rbeta(1000, 1.5, 5), rbeta(50, 5, 1.5))

# Fit multiple distribution families
models <- list()
for (family in c("gkw", "bkw", "kkw", "ekw", "mc", "kw", "beta")) {
  models[[family]] <- gkwfit(x, family = family, silent = TRUE, fit = "tmb", profile = TRUE, plot = TRUE)
}

# Compare models
model_comparison <- data.frame(
  Family = names(models),
  LogLik = sapply(models, logLik),
  AIC = sapply(models, AIC),
  BIC = sapply(models, BIC),
  Parameters = sapply(models, function(m) length(coef(m)))
)
print(model_comparison[order(model_comparison$AIC), ])

# Examine best model
best_family <- model_comparison$Family[which.min(model_comparison$AIC)]
summary(models[[best_family]])
models[[best_family]]$plots
```

## Practical Example

Let's fit some distributions to a simulated dataset:

```{r}
# Create artificial data sample in the (0,1) interval
require(patchwork)
set.seed(123)
data <- rbeta(2000, shape1 = 2, shape2 = 3)

# Fit models
fit_beta <- gkwfit(data, family = "beta", silent = TRUE, profile = TRUE, plot = TRUE)
fit_kw <- gkwfit(data, family = "kw", silent = TRUE, profile = TRUE, plot = TRUE)
fit_gkw <- gkwfit(data, family = "gkw", silent = TRUE, profile = TRUE, plot = TRUE)

# Summary of the best model (GKw)
summary(fit_gkw)

# Comparison via AIC
aic_values <- c(
  beta = AIC(fit_beta),
  kw = AIC(fit_kw),
  gkw = AIC(fit_gkw)
)
print(aic_values)

fit_beta$plots
fit_kw$plots
fit_gkw$plots
```

# Regression Modeling with gkwreg()

The `gkwreg()` function implements the GKw regression framework, allowing all five distribution parameters to be modeled as functions of covariates.

## Basic Usage

```{r, eval=FALSE}
# Simulate data
set.seed(789)
n <- 5000
x1 <- rnorm(n)
x2 <- runif(n, -1, 1)

# Generate parameters as functions of covariates
alpha <- exp(0.5 + 0.3 * x1)
beta <- exp(1.0 - 0.4 * x2)
gamma <- rep(1, n) # Fixed at 1
delta <- rep(0, n) # Fixed at 0
lambda <- rep(1, n) # Fixed at 1

# Generate response (simplified for this example - actually Kumaraswamy)
y <- (1 - (1 - runif(n))^(1 / beta))^(1 / alpha)

# Create data frame
data <- data.frame(y = y, x1 = x1, x2 = x2)

# Fit model (Kumaraswamy with covariates for alpha and beta)
model <- gkwreg(
  formula = y ~ x1 | x2 | 1 | 1 | 1,
  data = data,
  link = c(1, 1, 1, 1, 1), # All log links
  scale = c(10, 10, 10, 10, 10),
  silent = TRUE
)

# Examine model
summary(model)

# Diagnostic plots
plot(model, which = 1:6)

# Predictions
# new_data <- data.frame(x1 = c(-1, 2, 1), x2 = c(-0.5, 1, 0.5))
# predict(model, newdata = new_data, type = "response")
```

## Key Parameters

```r
gkwreg(formula, data, link = c(1, 1, 1, 1, 1), scale = c(10, 10, 10, 10, 10),
       start = NULL, control = list(), weights = NULL, subset = NULL,
       na.action = getOption("na.action", na.omit), offset = NULL,
       model = TRUE, x = TRUE, y = TRUE, silent = FALSE)
```

- `formula`: Formula object in the form `y ~ alpha_terms | beta_terms | gamma_terms | delta_terms | lambda_terms`
- `data`: Data frame containing the variables in the formula
- `link`: Numeric vector of length 5 specifying the link function for each parameter
- `scale`: Numeric vector of length 5 with scale factors for link functions 2-5
- `start`: Optional list with initial values for regression coefficients
- `control`: List of control parameters for optimization
- `offset`: Optional vector of offsets for the alpha parameter

## Complete Example: Modeling with Group Effects

```{r, eval=FALSE}
# Simulate data with varying shapes
set.seed(101)
n <- 1000
x <- runif(n, -2, 2)

# Create four groups with different distributions
group <- sample(1:4, n, replace = TRUE)
y <- numeric(n)

# Group 1: Right-skewed
idx <- which(group == 1)
y[idx] <- rbeta(length(idx), 1.5, 5)

# Group 2: Left-skewed
idx <- which(group == 2)
y[idx] <- rbeta(length(idx), 5, 1.5)

# Group 3: Symmetric
idx <- which(group == 3)
y[idx] <- rbeta(length(idx), 3, 3)

# Group 4: Bimodal
idx <- which(group == 4)
y[idx] <- c(rbeta(length(idx) / 2, 2, 8), rbeta(length(idx) / 2, 8, 2))

# Add continuous effect
y <- y * (0.8 + 0.2 * plogis(x))

# Create data frame with one-hot encoding for group
data <- data.frame(
  y = y,
  x = x,
  g2 = as.integer(group == 2),
  g3 = as.integer(group == 3),
  g4 = as.integer(group == 4)
)

# Fit model with group effects on all shape parameters
model <- gkwreg(
  formula = y ~ x | g2 + g3 + g4 | g2 + g3 + g4 | g2 + g3 + g4 | 1,
  data = data,
  link = c(1, 1, 1, 1, 1),
  silent = TRUE
)

# Examine results
summary(model)
```

## Simulated Example: Income Distribution by Education Level

Let's simulate income proportions (income divided by maximum income in the sample) by education level:

```{r}
# Simulate income proportion data (income/max_income)
set.seed(123)
n <- 500

# Create education level factor (1-4)
education <- sample(1:4, n, replace = TRUE)

# Simulate income proportion based on education level
y <- numeric(n)
for (i in 1:n) {
  # Higher education levels have higher mean income proportion
  # and different shapes of distribution
  if (education[i] == 1) {
    y[i] <- rbeta(1, 1.2, 4) # Lower education: right skewed, low mean
  } else if (education[i] == 2) {
    y[i] <- rbeta(1, 1.8, 3) # Some higher education: less skewed
  } else if (education[i] == 3) {
    y[i] <- rbeta(1, 2.5, 2.5) # Bachelor's: symmetric
  } else {
    y[i] <- rbeta(1, 4, 2) # Graduate degree: left skewed, high mean
  }
}

# Create additional covariates
age <- runif(n, 25, 65)
experience <- age - 20 - 2 * (education - 1) # Rough proxy for work experience

# Add age and experience effects
y <- y * (0.7 + 0.3 * plogis((age - 40) / 10)) # Age effect
y <- y * (0.8 + 0.2 * plogis((experience - 10))) # Experience effect

# Add random noise and ensure within (0,1)
y <- y * runif(n, 0.8, 1.2)
y <- pmin(pmax(y, 0.001), 0.999)

# Create data frame with dummy variables for education
data <- data.frame(
  y = y,
  age = age,
  experience = experience,
  ed2 = as.integer(education == 2),
  ed3 = as.integer(education == 3),
  ed4 = as.integer(education == 4)
)

# Fit GKw model
income_model <- gkwreg(
  y ~ age + experience + ed2 + ed3 + ed4 | ed2 + ed3 + ed4 | 1 | 1 | 1,
  data = data,
  link = c(1, 1, 1, 1, 1)
)

summary(income_model)

# Examine model diagnostics
par(mfrow = c(2, 2))
plot(income_model, which = 1:4)

# Create prediction grid for education levels
pred_data <- data.frame(
  age = rep(40, 4), # Set age to 40
  experience = rep(20, 4), # Set experience to 20 years
  ed2 = c(0, 1, 0, 0),
  ed3 = c(0, 0, 1, 0),
  ed4 = c(0, 0, 0, 1)
)

# Get predicted parameters
pred_params <- predict(income_model, type = "parameter")
# print(pred_params)

# Predicted means
pred_means <- predict(income_model, type = "response")
# print(pred_means)
```

# Advanced Topics

## Computational Considerations

1. **Starting Values**: For complex models, providing good starting values can significantly improve convergence:

```{r, eval=FALSE}
# Example with custom starting values
model <- gkwreg(
  formula = y ~ age | experience | ed2 | ed3 | ed4,
  data = data,
  start = list(
    beta1 = c(0.5, 0.3), # For alpha: intercept and x1 coefficient
    beta2 = c(1.0, -0.4), # For beta: intercept and x2 coefficient
    beta3 = c(.5, -2.0), # For gamma: intercept only
    beta4 = c(-0.5, 0.2), # For delta: intercept and x3 coefficient
    beta5 = c(0.1, -1) # For lambda: intercept only
  ), silent = TRUE,
  link = c(1, 1, 1, 1, 1)
)
```

2. **Optimization Control**: Tuning the optimization parameters can help with difficult convergence cases:

```{r, eval=FALSE}
# Example with custom control parameters
model <- gkwreg(
  formula = y ~ age | experience | ed2 | ed3 | ed4,
  data = data,
  control = list(
    optimizer = "nlminb", # Use nlminb optimizer
    eval.max = 10000, # Maximum function evaluations
    iter.max = 10000, # Maximum iterations
    rel.tol = 1e-10, # Relative tolerance
    abs.tol = 1e-15, # Absolute tolerance
    inner.method = "newton", # TMB internal method
    inner.maxit = 5000, # Maximum internal iterations
    smartsearch = TRUE, # Smart search in TMB
    openmp = TRUE # Use OpenMP if available
  ), silent = TRUE
)
```

3. **Link Function Selection**: Different link functions can be more appropriate for different parameters:

```{r, eval=FALSE}
# Example with different link functions for each parameter
model <- gkwreg(
  formula = y ~ x1 | x2 | 1 | x3 | 1,
  data = data,
  link = c(1, 2, 1, 7, 1), # log, logit, log, sqrt, log
  scale = c(10, 5, 10, 10, 10) # Scale factors for bounded links
)
```

## Residual Analysis

The `residuals()` function for `gkwreg` objects supports multiple types of residuals:

```{r, eval=FALSE}
# Types of residuals available
resid_types <- c(
  "response", "pearson", "deviance", "quantile",
  "modified.deviance", "cox-snell", "score"
)

# Generate and plot different residual types
par(mfrow = c(2, 2))
for (type in resid_types[1:4]) {
  resid <- residuals(model, type = type)
  plot(model$fitted.values, resid,
    main = paste(type, "residuals"),
    xlab = "Fitted values", ylab = "Residuals"
  )
  abline(h = 0, lty = 2, col = "red")
}
```

For diagnostic purposes, quantile residuals are often most appropriate for bounded data models as they approximate a standard normal distribution when the model is correctly specified.

## Model Comparisons

Nested models can be compared using likelihood ratio tests:

```{r, eval=FALSE}
# Full model with covariates for all parameters
model_full <- gkwreg(y ~ x1 + x2 | x1 + x2 | x1 + x2 | x1 + x2 | x1 + x2, data = data)

# Reduced model with covariates only for alpha and beta
model_reduced <- gkwreg(y ~ x1 + x2 | x1 + x2 | 1 | 1 | 1, data = data)

# Likelihood ratio test
lr_stat <- 2 * (as.numeric(logLik(model_full)) - as.numeric(logLik(model_reduced)))
df_diff <- model_full$npar - model_reduced$npar
p_value <- pchisq(lr_stat, df = df_diff, lower.tail = FALSE)

cat("LR statistic:", lr_stat, "\n")
cat("Degrees of freedom:", df_diff, "\n")
cat("p-value:", p_value, "\n")
```

Non-nested models can be compared using information criteria:

```{r, eval=FALSE}
# Compare different specifications using AIC/BIC
model_log <- gkwreg(y ~ x | 1 | 1 | 1 | 1, data = data, link = c(1, 1, 1, 1, 1))
model_logit <- gkwreg(y ~ x | 1 | 1 | 1 | 1, data = data, link = c(2, 1, 1, 1, 1))

AIC(model_log, model_logit)
BIC(model_log, model_logit)
```

# Implementation Details

## Parameter Estimation

The `gkwfit()` function implements maximum likelihood estimation (MLE) for the GKw distribution family using either TMB or Newton-Raphson:

1. **TMB Method**: Uses automatic differentiation for highly efficient gradient and Hessian computation.
   
   ```r
   # TMB implementation example
   TMB::MakeADFun(
     data = tmb_data,
     parameters = params,
     DLL = dll_name,
     silent = control$tmb.silent,
     method = control$inner.method,
     hessian = control$hessian
   )
   ```

2. **Newton-Raphson Method**: Direct implementation using analytical gradients and Hessian.

   ```r
   # Gradient calculation for parameter alpha using GKw distribution
   grad_alpha <- n/alpha + sum(log(x)) - 
     (beta-1)*sum(x^alpha*log(x)/(1-x^alpha)) + 
     (gamma*lambda-1)*sum(beta*(1-x^alpha)^(beta-1)*x^alpha*log(x)/(1-(1-x^alpha)^beta)) - 
     delta*sum(lambda*(1-(1-x^alpha)^beta)^(lambda-1)*beta*(1-x^alpha)^(beta-1)*x^alpha*log(x)/(1-(1-(1-x^alpha)^beta)^lambda))
   ```

## Diagnostic Tools

The package provides comprehensive diagnostic tools:

1. **P-P and Q-Q Plots**: For assessing distributional fit
2. **Residual Plots**: Multiple residual types to check model assumptions
3. **Profile Likelihood Plots**: For uncertainty visualization
4. **Leverage and Influence Diagnostics**: To identify influential observations

## Numerical Stability

The implementation includes several features to enhance numerical stability:

1. **Boundary Handling**: Automatic adjustment of observations near 0 or 1
2. **Parameter Constraints**: Enforcement of positivity constraints
3. **Optimization Safeguards**: Step size control and parameter scaling
4. **Log Transformation**: Working with log-parameters to ensure positivity

# Best Practices and Recommendations

1. **Model Building Strategy**: 
   - Start with simpler models (fewer covariates, fewer parameters) and gradually increase complexity
   - Test for the need of the full 5-parameter GKw distribution by comparing with simpler subfamilies
   - Consider different link functions for different parameters

2. **Numerical Stability**:
   - Ensure data is strictly within (0, 1) - the package handles this automatically but preprocessing can be helpful
   - Scale covariates to similar ranges to improve optimization
   - Start with log links before trying more complex link functions

3. **Interpretation**:
   - The parameters have different interpretations in the GKw distribution:
     - α and β: Primary shape parameters (similar to Kumaraswamy)
     - γ and δ: Secondary shape parameters (similar to Beta)
     - λ: Additional shape parameter
   - The mean of the distribution is a complex function of all parameters, so direct parameter interpretation can be challenging
   - Use the `predict()` function with `type = "response"` to estimate conditional means

4. **Performance Considerations**:
   - For large datasets, ensure sufficient memory is available
   - Consider using OpenMP parallelization (set `control = list(openmp = TRUE)`)
   - For complex models, set `silent = TRUE` to reduce output verbosity

# References

Cordeiro, G. M., & de Castro, M. (2011). A new family of generalized distributions. *Journal of Statistical Computation and Simulation*, 81(7), 883-898.

Kumaraswamy, P. (1980). A generalized probability density function for double-bounded random processes. *Journal of Hydrology*, 46(1-2), 79-88.

Jones, M. C. (2009). Kumaraswamy's distribution: A beta-type distribution with some tractability advantages. *Statistical Methodology*, 6(1), 70-81.

Kristensen, K., Nielsen, A., Berg, C. W., Skaug, H., & Bell, B. M. (2016). TMB: Automatic differentiation and Laplace approximation. *Journal of Statistical Software*, 70(5), 1-21.

Cribari-Neto, F., & Zeileis, A. (2010). Beta regression in R. *Journal of Statistical Software*, 34(2), 1-24.

Ferrari, S. L. P., & Cribari-Neto, F. (2004). Beta regression for modelling rates and proportions. *Journal of Applied Statistics*, 31(7), 799-815.

Smithson, M., & Verkuilen, J. (2006). A better lemon squeezer? Maximum-likelihood regression with beta-distributed dependent variables. *Psychological Methods*, 11(1), 54-71.
