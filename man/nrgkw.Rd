% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{nrgkw}
\alias{nrgkw}
\title{Newton-Raphson Optimization for Kumaraswamy Family Distributions}
\usage{
nrgkw(
  start,
  data,
  family = "gkw",
  tol = 0.000001,
  max_iter = 100L,
  verbose = FALSE,
  use_hessian = TRUE,
  step_size = 1,
  enforce_bounds = TRUE,
  min_param_val = 0.00001,
  max_param_val = 100000,
  get_num_hess = FALSE
)
}
\arguments{
\item{start}{A numeric vector containing initial values for parameters, with length
corresponding to the selected family (see Details)}

\item{data}{A numeric vector containing observed data. All values must be in the interval (0,1)}

\item{family}{A character string specifying the distribution family. One of "gkw", "bkw", "kkw",
"ekw", "mc", "kw", or "beta". Default: "gkw"}

\item{tol}{Convergence tolerance. The algorithm stops when the gradient norm or parameter/likelihood
changes are below this value. Default: 1e-6}

\item{max_iter}{Maximum number of iterations. Default: 100}

\item{verbose}{Logical flag to print detailed progress information. Default: FALSE}

\item{use_hessian}{Logical flag to use Hessian information for parameter updates. If FALSE,
the algorithm uses gradient descent instead. Default: TRUE}

\item{step_size}{Initial step size for parameter updates. Default: 1.0}

\item{enforce_bounds}{Logical flag to enforce parameter constraints. Default: TRUE}

\item{min_param_val}{Minimum allowed value for parameters (except delta). Default: 1e-5}

\item{max_param_val}{Maximum allowed value for parameters. Default: 1e5}

\item{get_num_hess}{Logical flag to calculate numerical Hessian in addition to analytical Hessian.
Default: FALSE}
}
\value{
A list containing the following components:
\describe{
\item{parameters}{A numeric vector with the estimated parameters}
\item{loglik}{The maximized log-likelihood value}
\item{iterations}{Number of iterations performed}
\item{converged}{Logical flag indicating whether the algorithm converged}
\item{param_history}{Matrix of parameter values at each iteration}
\item{loglik_history}{Vector of log-likelihood values at each iteration}
\item{gradient}{The gradient vector at the final parameter estimates}
\item{hessian}{The Hessian matrix at the final parameter estimates}
\item{std_errors}{Standard errors for the estimated parameters}
\item{aic}{Akaike Information Criterion: AIC = 2k - 2ln(L)}
\item{bic}{Bayesian Information Criterion: BIC = k ln(n) - 2ln(L)}
\item{n}{Sample size}
\item{status}{Character string indicating the termination status}
\item{z_values}{Z-statistics for parameter significance tests}
\item{p_values}{P-values for parameter significance tests}
\item{param_names}{Character vector of parameter names}
\item{family}{The distribution family used in the estimation}
\item{numeric_hessian}{Numerical approximation of the Hessian (only if get_num_hess=TRUE)}
}
}
\description{
Performs maximum likelihood estimation for the parameters of any distribution in the
Generalized Kumaraswamy (GKw) family using a robust implementation of the Newton-Raphson
algorithm. This function supports all 7 submodels: GKw, BKw, KKw, EKw, Mc (McDonald),
Kw, and Beta.
}
\details{
The Generalized Kumaraswamy family includes the following distributions, all defined on (0,1):
\itemize{
\item \strong{GKw} (Generalized Kumaraswamy): 5 parameters (α, β, γ, δ, λ)
\item \strong{BKw} (Beta-Kumaraswamy): 4 parameters (α, β, γ, δ), with λ = 1 fixed
\item \strong{KKw} (Kumaraswamy-Kumaraswamy): 4 parameters (α, β, δ, λ), with γ = 1 fixed
\item \strong{EKw} (Exponentiated Kumaraswamy): 3 parameters (α, β, λ), with γ = 1, δ = 0 fixed
\item \strong{Mc} (McDonald/Beta Power): 3 parameters (γ, δ, λ), with α = 1, β = 1 fixed
\item \strong{Kw} (Kumaraswamy): 2 parameters (α, β), with γ = 1, δ = 0, λ = 1 fixed
\item \strong{Beta}: 2 parameters (γ, δ), with α = 1, β = 1, λ = 1 fixed
}

This function implements a sophisticated optimization procedure to find the maximum likelihood
estimates with multiple fallback strategies to handle numerical challenges:
\enumerate{
\item Cholesky decomposition (fastest, requires positive-definite Hessian)
\item Standard matrix solver
\item Regularized Hessian with incremental adjustment
\item Pseudo-inverse for highly ill-conditioned matrices
\item Gradient descent as ultimate fallback
}

The function also implements backtracking line search to ensure monotonic improvement in the
log-likelihood, with random parameter perturbation as a recovery strategy when backtracking fails.
}
\section{Warning}{

Convergence is not guaranteed for all datasets and initial values. It's recommended to:
\itemize{
\item Try different initial values if convergence fails
\item Check the gradient norm at the final solution to verify optimality
\item Examine parameter history to identify potential issues
\item Use the verbose option to get detailed progress information for troubleshooting
}
}

\examples{
\dontrun{
# Generate sample data from Beta(2,5) distribution for testing
set.seed(123)
sample_data <- rbeta(200, 2, 5)

# Fit with full GKw model
gkw_result <- mle_fit(c(1.5, 4.5, 1.0, 0.0, 1.0), sample_data, family = "gkw")
gkw_result$parameters

# Fit with simpler Kumaraswamy model
kw_result <- mle_fit(c(1.5, 4.5), sample_data, family = "kw")
kw_result$parameters

# Fit with Beta model
beta_result <- mle_fit(c(2.0, 5.0), sample_data, family = "beta")
beta_result$parameters

# Compare AIC/BIC values to select the best model
data.frame(
  family = c("gkw", "kw", "beta"),
  AIC = c(gkw_result$aic, kw_result$aic, beta_result$aic),
  BIC = c(gkw_result$bic, kw_result$bic, beta_result$bic)
)
}

}
\references{
Kumaraswamy, P. (1980). A generalized probability density function for double-bounded
random processes. Journal of Hydrology, 46(1-2), 79-88.

Cordeiro, G. M., & de Castro, M. (2011). A new family of generalized distributions.
Journal of Statistical Computation and Simulation, 81(7), 883-898.

Fletcher, R. (1987). Practical Methods of Optimization. John Wiley & Sons.
}
\author{
Lopes, J. E.
}
